{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anthony Wilson\n",
    "##### Week 6 Problem 1\n",
    "##### 7/18/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this part is to write a script to compute the decision tree for an input dataset. You can assume that all attributes are numeric, except for the last attribute which is the class.\n",
    "\n",
    "You should use the Information Gain based on Entropy for computing the best split value for each attribute. For the stopping criteria at a node, stop if the purity is at least 95% or stop if the node size is five or lower.\n",
    "\n",
    "Note that the best way to implement the splits for numeric attributes is to sort the values of that attribute from smallest to largest. Then you can use the mid-point between two distinct (consecutive) values as the split test of the form A≤v. You can then update the class frequencies on both sides of the split and compute the split entropy for each decision. After comparing with the entropy for the node, you can find the best split for the current attribute. Now repeat the whole process for each numeric attribute at that node, and choose the best split over all attributes. Finally, split the data according to the best split, and repeat the whole method recursively, until the stopping conditions are met.\n",
    "\n",
    "The decision tree should be printed in the following format:\n",
    "\n",
    "Decision: Car <= 1.5 , Gain= 0.4591479\n",
    "\n",
    "Decision: Age <= 22.5 , Gain= 0.9182958\n",
    "\n",
    "Leaf: label= H purity= 1 size= 1\n",
    "\n",
    "Leaf: label= L purity= 1 size= 2\n",
    "\n",
    "Leaf: label= H purity= 1 size= 3\n",
    "\n",
    "Note that each internal node, print the decision followed by the Information Gain, and for each leaf, print the majority label, purity of the leaf, and the size. The indentation indicates the tree level. All nodes at the same level of indentation (tabs) are at the same level in the tree. For the tree above, Car<=1.5 is the root decision. Its left child is Age<=22.5, and its right child is a leaf. Also, for Age≤22.5 its left and right children appear immediately below it.\n",
    "\n",
    "You may test your program on the iris.txt dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Iris-versicolor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width            label\n",
       "0           5.9          3.0           4.2          1.5  Iris-versicolor\n",
       "1           6.9          3.1           4.9          1.5  Iris-versicolor\n",
       "2           6.6          2.9           4.6          1.3  Iris-versicolor\n",
       "3           4.6          3.2           1.4          0.2      Iris-setosa\n",
       "4           6.0          2.2           4.0          1.0  Iris-versicolor"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "\n",
    "def readInTextFile(filename):\n",
    "    \"\"\"\n",
    "    Reading in file and returning the results\n",
    "    \"\"\"\n",
    "    _list_ = []\n",
    "    i = 0\n",
    "    # using 'with open' function, so that when finished looping file it closes\n",
    "    with open(filename, 'r') as file:\n",
    "        #loop through line by line to read file\n",
    "        for line in file:\n",
    "            # strip all extra unnecesary text like drop line \"\\n\" \n",
    "            # use split to break up the values into a list\n",
    "            s = line.strip().split( \",\")\n",
    "            _list_.append(s)\n",
    "    return _list_\n",
    "# bring in file data as a numpy list so that it is easier to slice \n",
    "data = pd.DataFrame(readInTextFile(\"iris.txt\"), columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'label'])\n",
    "# convert strings to floats \n",
    "for col in data.columns[:-1]:\n",
    "    data[col] = data[col].astype(float)\n",
    "# drop quoutes in label columns\n",
    "data['label'] = data['label'].str.replace('\"', '')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size):\n",
    "    \"\"\"\n",
    "    train_test_split:\n",
    "    Takes a pandas dataframe and splits it up into a training set and a test set, \n",
    "    based on the test size. It will then return the two dataframes.\n",
    "    \"\"\"\n",
    "    # when test_size represents a probability between 0 and 1 resets test_size \n",
    "    # by the proportion sent to the function\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(df))\n",
    "    # get a list of all the indexes    \n",
    "    indices = df.index.to_list()\n",
    "    # get a sample from the dataframe for the test data set\n",
    "    test_indices= rd.sample(population = indices, k = test_size)\n",
    "    # pull indices from dataframe and create the test dataframe\n",
    "    test_df = df.loc[test_indices]\n",
    "    # drop test dataframe indices for the training dataset\n",
    "    train_df = df.drop(test_indices)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_purity(data):\n",
    "    \"\"\"\n",
    "    calc_purity:\n",
    "    Checks the purity of the data, which is a numpy array and returns if it is pure or not. \n",
    "    If the max probabilty of the number of labels from each class returns higher then the \n",
    "    given threshold return true. \n",
    "    \"\"\"\n",
    "    labels = data[:,-1]\n",
    "    counts = np.unique(labels, return_counts=True)[1]\n",
    "\n",
    "    probabilities = counts/counts.sum()\n",
    "    return max(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_data(data):\n",
    "    \"\"\"\n",
    "    classify_data:\n",
    "    Takes numpy array data and returns label with the most counts in the data set. \n",
    "    \"\"\"\n",
    "    # Gather labels\n",
    "    labels = data[:,-1]\n",
    "    # Get each unique class and their counts\n",
    "    uniq_classes, uniq_class_counts = np.unique(labels, return_counts=True)\n",
    "    # Get the index with the most labels from uniq_class_counts\n",
    "    index = uniq_class_counts.argmax()\n",
    "    # Get label name\n",
    "    classification = uniq_classes[index]\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_potential_splits(data):\n",
    "    \"\"\"\n",
    "    get_potential_splits:\n",
    "    For each column in the numpy array, besides the last one which is the label, \n",
    "    return a dictionary of all possibible splits. The potential_split is calculated over \n",
    "    a loop where the previous value within the column is added to the current and divided by \n",
    "    two. This is done for each column over earch row. \n",
    "    \"\"\"\n",
    "    # initiate potential_splits\n",
    "    potential_splits= {}\n",
    "    # Get the number of columns from the data set\n",
    "    n_columns = data.shape[1]\n",
    "    \n",
    "    # For each column, besides the label create list of unique values\n",
    "    for column_index in range(n_columns-1):\n",
    "        potential_splits[column_index] = []\n",
    "        values = data[:,column_index]\n",
    "        unique_values = np.unique(values)\n",
    "        # skip the first index of each column you can't split the first and the last\n",
    "        for index in range(1,len(unique_values)):\n",
    "            current_value = unique_values[index]\n",
    "            previous_value = unique_values[index-1]\n",
    "            potential_split = (current_value+previous_value)/2\n",
    "            potential_splits[column_index].append(potential_split)\n",
    "    \n",
    "    return potential_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, split_column, split_value):\n",
    "    \"\"\"\n",
    "    split_data:\n",
    "    Takes the numpy array data and splits based on the column index\n",
    "    and a value pased through the function. It will create two new \n",
    "    numpy arrays based on the split value sent. There will two arrays \n",
    "    returned one arrayof data below the split value and one above split \n",
    "    value.\n",
    "    \"\"\"    \n",
    "    split_col_values = data[:,split_column]\n",
    "    data_below = data[split_col_values <= split_value]\n",
    "    data_above = data[split_col_values > split_value]\n",
    "    return data_below, data_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(data):\n",
    "    \"\"\"\n",
    "    calc_entropy:\n",
    "    Takes numpy array data and calculates the entropy. First it pulls the\n",
    "    labels and counts, then it calculates the probabilities for each, \n",
    "    then it calculates the entropy by summing the probability times the \n",
    "    negative log of the probabilities and returns the value.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels = data[:,-1]\n",
    "    counts = np.unique(labels, return_counts=True)[1]\n",
    "\n",
    "    probabilities = counts/counts.sum()\n",
    "    impurity = sum(probabilities * -np.log2(probabilities))\n",
    "    return impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_overall_entropy(data_below, data_above):\n",
    "    \"\"\"\n",
    "    calc_overall_entropy:\n",
    "    Calculates the overall entropy for the data given the data below a certain value\n",
    "    within a column and above that value. First we get the total length of each numpy \n",
    "    array passed for data above and below, then we calulate the probability for both\n",
    "    and take the probability times the entropy of each and some them up to get the \n",
    "    overall entropy and return the value.\n",
    "    \"\"\"\n",
    "    n_data = len(data_below) + len(data_above)\n",
    "\n",
    "    p_data_below = len(data_below) / n_data\n",
    "    p_data_above = len(data_above) / n_data\n",
    "    overall_entropy =( p_data_below * calc_entropy(data_below)\n",
    "                     +  p_data_above * calc_entropy(data_above))\n",
    "    \n",
    "    return overall_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_best_split(data,potential_splits):\n",
    "    \"\"\"\n",
    "    determine_best_split:\n",
    "    This goes through the numpy array data with all potential splits from the data\n",
    "    based on column and each unique value within, and returns the best possible split\n",
    "    by calculating the overall entropy if the data was to be split by the column and split value. \n",
    "    The function loops over each column and if the overall entropy is the smallest, the column and \n",
    "    split value is stored and returned. \n",
    "    \"\"\"\n",
    "    # information gain set to 0 to initialize\n",
    "    best_gain = 0\n",
    "    # Get current uncertainty for data to measure information gain\n",
    "    current_uncertainty = calc_entropy(data)\n",
    "    \n",
    "    # loop through each column and value to determine best split\n",
    "    for column_index in potential_splits:\n",
    "        for value in potential_splits[column_index]:\n",
    "            # Split the data based on the column and value\n",
    "            data_below, data_above = split_data(data,column_index, value)\n",
    "            # Get the current iterations uncertainty\n",
    "            iteration_uncertainty = calc_overall_entropy(data_below, data_above)\n",
    "            # Calculate the new information gain \n",
    "            gain = current_uncertainty - iteration_uncertainty\n",
    "            # If the current iteration increases the information gain update best values\n",
    "            if gain >= best_gain:\n",
    "                best_gain = gain\n",
    "                best_split_column = column_index\n",
    "                best_split_value = value\n",
    "    return best_split_column, best_split_value, best_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_algorithm(df, counter= 0, min_node = 5, threshold_purity = .95):\n",
    "    \"\"\"\n",
    "    decision_tree_algorithm:\n",
    "    This is a recursive function that takes a panda dataframe and determines the best way to split the \n",
    "    data. It first converts the dataframe into a numpy array, checks to see if it is pure if it is not\n",
    "    then it will start to find the best split among all columns. It calls itself instead of using \n",
    "    a while loop. It will continue to recursively call its self until each node is pure. \n",
    "    \"\"\"    \n",
    "    # First step, if the counter=1 then setup the data set otherwise the data is already setup \n",
    "    # as a numpy array.\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df\n",
    "\n",
    "    purity = calc_purity(data)\n",
    "    node_size = len(data)\n",
    "    # Check to see if the data is pure, if it is stop and return the classification to create a leaf.\n",
    "    if (purity >= threshold_purity) or (node_size < min_node):\n",
    "        classification = classify_data(data)\n",
    "        leaf = 'Leaf: label = {} purity = {} size = {}'.format(classification, purity, node_size)\n",
    "        return leaf\n",
    "    # recursive part\n",
    "    else:\n",
    "        counter += 1\n",
    "        \n",
    "        # helper functions\n",
    "        potential_splits = get_potential_splits(data)\n",
    "        split_column, split_value, information_gain = determine_best_split(data, potential_splits)\n",
    "        data_below, data_above = split_data(data, split_column, split_value)\n",
    "        \n",
    "        # instantiate sub-tree\n",
    "        feature_name = COLUMN_HEADERS[split_column]\n",
    "        question = \"Decision: {} <= {} , Gain = {}\".format( feature_name, split_value, information_gain)\n",
    "        sub_tree = {question: []}\n",
    "        \n",
    "        # find answers (recursion)\n",
    "        left_node = decision_tree_algorithm(data_below, counter, min_node, threshold_purity)\n",
    "        right_node = decision_tree_algorithm(data_above, counter, min_node,threshold_purity)\n",
    "        \n",
    "        if left_node == right_node:\n",
    "            sub_tree = left_node\n",
    "        else: \n",
    "            sub_tree[question].append(left_node)\n",
    "            sub_tree[question].append(right_node)\n",
    "        \n",
    "        return sub_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_example(example,tree):\n",
    "    \"\"\"\n",
    "    classify_example:\n",
    "    Takes a value from the data as list with column values and recursively goes through the decision tree\n",
    "    to classify each row. \n",
    "    \"\"\"\n",
    "    # Pull inequality with columns\n",
    "    q = list(tree.keys())[0]\n",
    "    # Get the subtree index by subtracting 1 from True or False to get the inverse\n",
    "    sub_tree_index = 1-int(example[q.split(' ')[1]] <= float(q.split(' ')[3]))\n",
    "    # Get the index\n",
    "    response = list(tree.values())[0][sub_tree_index]\n",
    "    # If the response is not a dictionary return the repsonse\n",
    "    if type(response) != dict:\n",
    "        return response.split(' ')[3]\n",
    "    # Else work down the decision tree recurrsively\n",
    "    else:\n",
    "        return classify_example(example, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df, tree):\n",
    "    \"\"\"\n",
    "    calculate_accuracy:\n",
    "    Takes the dataframe sent and classifies each row. Then it compares the classification\n",
    "    to the actual label and calculates the accuacy. \n",
    "    \"\"\"\n",
    "    df['classification'] = df.apply(classify_example, axis= 1, args = (tree,))\n",
    "    df['class_true'] = df.classification == df.label\n",
    "    accuracy = df['class_true'].sum()/len(df)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_decision_tree(tree, tabs = 0):\n",
    "    \"\"\"\n",
    "    print_decision_tree:\n",
    "    Takes the decision tree and makes it readable for the user with tabs based on what tree we are on. \n",
    "    It will display the decision and leaf node. \n",
    "    \"\"\"\n",
    "    # tab is used to determine the number of tabs to insert based on what part of the sub tree we are on\n",
    "    for decision,leafs in tree.items():\n",
    "        print('{}{}'.format('\\t' * tabs, decision))\n",
    "        tabs += 1    \n",
    "        for leaf in leafs:\n",
    "            if type(leaf) != dict:\n",
    "                print('{}{}'.format('\\t' * tabs,leaf))\n",
    "            else:\n",
    "                print_decision_tree(leaf, tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The decision tree was 96.66666666666667% accurate.\n",
      "\n",
      "Decision: petal_width <= 0.8 , Gain = 0.941291028227705\n",
      "\tLeaf: label = Iris-setosa purity = 1.0 size = 43\n",
      "\tDecision: petal_width <= 1.75 , Gain = 0.7140999395534506\n",
      "\t\tDecision: petal_length <= 4.95 , Gain = 0.14962749466371295\n",
      "\t\t\tLeaf: label = Iris-versicolor purity = 0.972972972972973 size = 37\n",
      "\t\t\tLeaf: label = Iris-virginica purity = 0.6666666666666666 size = 3\n",
      "\t\tLeaf: label = Iris-virginica purity = 0.972972972972973 size = 37\n"
     ]
    }
   ],
   "source": [
    "# get training and test dataframes\n",
    "train_df, test_df = train_test_split(data, .2)\n",
    "tree = decision_tree_algorithm(train_df)\n",
    "print('The decision tree was {}% accurate.\\n'.format(calculate_accuracy(test_df,tree)*100))\n",
    "print_decision_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
